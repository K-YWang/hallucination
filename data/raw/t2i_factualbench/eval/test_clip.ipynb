{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "from typing import Literal, TypeAlias\n",
    "\n",
    "import megfile\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import PIL.ImageOps\n",
    "import requests\n",
    "import torch\n",
    "import functools\n",
    "import torch.distributed as dist\n",
    "\n",
    "# from .loguru import logger\n",
    "\n",
    "\"\"\"\n",
    "- pil: `PIL.Image.Image`, size (w, h), seamless conversion between `uint8`\n",
    "- np: `np.ndarray`, shape (h, w, c), default `np.uint8`\n",
    "- pt: `torch.Tensor`, shape (c, h, w), default `torch.uint8`\n",
    "\"\"\"\n",
    "ImageType: TypeAlias = PIL.Image.Image | np.ndarray | torch.Tensor\n",
    "ImageTypeStr: TypeAlias = Literal[\"pil\", \"np\", \"pt\"]\n",
    "ImageFormat: TypeAlias = Literal[\"JPEG\", \"PNG\"]\n",
    "DataFormat: TypeAlias = Literal[\"255\", \"01\", \"11\"]\n",
    "\n",
    "\n",
    "IMAGE_EXT_LOWER = [\"png\", \"jpeg\", \"jpg\"]\n",
    "IMAGE_EXT = IMAGE_EXT_LOWER + [_ext.upper() for _ext in IMAGE_EXT_LOWER]\n",
    "\n",
    "\n",
    "def check_image_type(image: ImageType):\n",
    "    if not (isinstance(image, PIL.Image.Image) or isinstance(image, np.ndarray) or isinstance(image, torch.Tensor)):\n",
    "        raise TypeError(f\"`image` should be PIL Image, ndarray or Tensor. Got `{type(image)}`.\")\n",
    "\n",
    "\n",
    "def load_image(\n",
    "    image: str | os.PathLike | PIL.Image.Image,\n",
    "    *,\n",
    "    output_type: ImageTypeStr = \"pil\",\n",
    ") -> ImageType:\n",
    "    \"\"\"\n",
    "    Loads `image` to a PIL Image, NumPy array or PyTorch tensor.\n",
    "\n",
    "    Args:\n",
    "        image (str | PIL.Image.Image): The path to image or PIL Image.\n",
    "        mode (ImageMode, optional): The mode to convert to. Defaults to None (no conversion).\n",
    "            The current version supports all possible conversions between \"L\", \"RGB\", \"RGBA\".\n",
    "        output_type (ImageTypeStr, optional): The type of the output image. Defaults to \"pil\".\n",
    "            The current version supports \"pil\", \"np\", \"pt\".\n",
    "\n",
    "    Returns:\n",
    "        ImageType: The loaded image in the given type.\n",
    "    \"\"\"\n",
    "    timeout = 10\n",
    "    # Load the `image` into a PIL Image.\n",
    "    if isinstance(image, str) or isinstance(image, os.PathLike):\n",
    "        if image.startswith(\"http://\") or image.startswith(\"https://\"):\n",
    "            try:\n",
    "                image = PIL.Image.open(requests.get(image, stream=True, timeout=timeout).raw)\n",
    "            except requests.exceptions.Timeout:\n",
    "                raise ValueError(f\"HTTP request timed out after {timeout} seconds\")\n",
    "        elif image.startswith(\"s3\"):\n",
    "            with megfile.smart_open(image, \"rb\") as f:\n",
    "                bytes_data = f.read()\n",
    "            image = PIL.Image.open(io.BytesIO(bytes_data), \"r\")\n",
    "        elif os.path.isfile(image):\n",
    "            image = PIL.Image.open(image)\n",
    "        else:\n",
    "            raise ValueError(f\"Incorrect path or url, URLs must start with `http://`, `https://` or `s3+[profile]://`, and `{image}` is not a valid path.\")\n",
    "    elif isinstance(image, PIL.Image.Image):\n",
    "        image = image\n",
    "    else:\n",
    "        raise ValueError(f\"`image` must be a path or PIL Image, got `{type(image)}`\")\n",
    "\n",
    "    # Automatically adjust the orientation of the image to match the direction it was taken.\n",
    "    image = PIL.ImageOps.exif_transpose(image)\n",
    "\n",
    "    support_mode = [\"L\", \"RGB\", \"RGBA\", \"CMYK\"]\n",
    "    if image.mode not in support_mode:\n",
    "        raise ValueError(f\"Only support mode in `{support_mode}`, got `{image.mode}`\")\n",
    "\n",
    "    # add white background for RGBA images, and convert to RGB\n",
    "    if image.mode == \"RGBA\":\n",
    "        background = PIL.Image.new(\"RGBA\", image.size, \"white\")\n",
    "        image = PIL.Image.alpha_composite(background, image).convert(\"RGB\")\n",
    "\n",
    "    image = image.convert(\"RGB\")\n",
    "\n",
    "    if output_type == \"pil\":\n",
    "        image = image\n",
    "    elif output_type == \"np\":\n",
    "        image = to_np(image)\n",
    "    elif output_type == \"pt\":\n",
    "        image = to_pt(image)\n",
    "    else:\n",
    "        raise ValueError(f\"`output_type` must be one of `{ImageTypeStr}`, got `{output_type}`\")\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "@functools.lru_cache()\n",
    "def _get_global_gloo_group():\n",
    "    \"\"\"\n",
    "    Return a process group based on gloo backend, containing all the ranks\n",
    "    The result is cached.\n",
    "    \"\"\"\n",
    "    if dist.get_backend() == \"nccl\":\n",
    "        return dist.new_group(backend=\"gloo\")\n",
    "    else:\n",
    "        return dist.group.WORLD\n",
    "\n",
    "\n",
    "def all_gather(data, group=None):\n",
    "    \"\"\"\n",
    "    Run all_gather on arbitrary picklable data (not necessarily tensors).\n",
    "\n",
    "    Args:\n",
    "        data: any picklable object\n",
    "        group: a torch process group. By default, will use a group which\n",
    "            contains all ranks on gloo backend.\n",
    "\n",
    "    Returns:\n",
    "        list[data]: list of data gathered from each rank\n",
    "    \"\"\"\n",
    "    if group is None:\n",
    "        group = _get_global_gloo_group()  # use CPU group by default, to reduce GPU RAM usage.\n",
    "    world_size = dist.get_world_size(group)\n",
    "    if world_size == 1:\n",
    "        return [data]\n",
    "\n",
    "    device = data.device\n",
    "    output = [None for _ in range(world_size)]\n",
    "    dist.all_gather_object(output, data, group=group)\n",
    "    output = [o.to(device) for o in output]\n",
    "    return output\n",
    "\n",
    "\n",
    "def initialize_distributed_backend(backend=\"nccl\", init_method=\"env://\"):\n",
    "    \"\"\"\n",
    "    Initializes the default distributed process group.\n",
    "    Args:\n",
    "        backend (str): Backend to use (nccl, gloo).\n",
    "        init_method (str): URL specifying how to initialize the process group.\n",
    "    \"\"\"\n",
    "    dist.init_process_group(backend=backend, init_method=init_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/sd3_hzw/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Evaluating CLIP-T Score: 100%|██████████| 200/200 [00:17<00:00, 11.34it/s]\n",
      "Evaluating CLIP-I Score: 100%|██████████| 200/200 [00:30<00:00,  6.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clipt_score: 34.99002919197083\n",
      "clipi_score: 73.49842475891113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating CLIP-T Score: 100%|██████████| 100/100 [00:06<00:00, 15.40it/s]\n",
      "Evaluating CLIP-I Score: 100%|██████████| 100/100 [00:11<00:00,  8.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clipt_score: 35.40941041946411\n",
      "clipi_score: 73.49188335418701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating CLIP-T Score: 100%|██████████| 100/100 [00:06<00:00, 15.24it/s]\n",
      "Evaluating CLIP-I Score: 100%|██████████| 100/100 [00:13<00:00,  7.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clipt_score: 36.39902774810791\n",
      "clipi_score: 79.83686378479004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating CLIP-T Score: 100%|██████████| 100/100 [00:06<00:00, 15.25it/s]\n",
      "Evaluating CLIP-I Score: 100%|██████████| 100/100 [00:09<00:00, 10.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clipt_score: 36.732144584655764\n",
      "clipi_score: 79.03935253143311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating CLIP-T Score: 100%|██████████| 150/150 [00:10<00:00, 14.50it/s]\n",
      "Evaluating CLIP-I Score: 100%|██████████| 150/150 [00:16<00:00,  9.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clipt_score: 33.36951310475667\n",
      "clipi_score: 72.28870056152344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating CLIP-T Score: 100%|██████████| 150/150 [00:09<00:00, 15.36it/s]\n",
      "Evaluating CLIP-I Score: 100%|██████████| 150/150 [00:28<00:00,  5.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clipt_score: 34.64983894348145\n",
      "clipi_score: 73.21331253051758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 测试样本\n",
    "\n",
    "import math\n",
    "import os\n",
    "from typing import Literal, List\n",
    "# import fire\n",
    "# import megfile\n",
    "import torch\n",
    "import PIL.Image\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "import glob\n",
    "from typing import Literal, TypeAlias\n",
    "import json\n",
    "\n",
    "# 定义常量\n",
    "_DEFAULT_MODEL = \"/mnt/workspace/ziwei/checkpoints/clip-vit-base-patch32\"\n",
    "_DEFAULT_TORCH_DTYPE: torch.dtype = torch.float32\n",
    "IMAGE_EXT_LOWER = [\"png\", \"jpeg\", \"jpg\"]\n",
    "IMAGE_EXT = IMAGE_EXT_LOWER + [_ext.upper() for _ext in IMAGE_EXT_LOWER]\n",
    "\n",
    "\"\"\"\n",
    "- pil: `PIL.Image.Image`, size (w, h), seamless conversion between `uint8`\n",
    "- np: `np.ndarray`, shape (h, w, c), default `np.uint8`\n",
    "- pt: `torch.Tensor`, shape (c, h, w), default `torch.uint8`\n",
    "\"\"\"\n",
    "ImageType: TypeAlias = PIL.Image.Image | np.ndarray | torch.Tensor\n",
    "ImageTypeStr: TypeAlias = Literal[\"pil\", \"np\", \"pt\"]\n",
    "ImageFormat: TypeAlias = Literal[\"JPEG\", \"PNG\"]\n",
    "DataFormat: TypeAlias = Literal[\"255\", \"01\", \"11\"]\n",
    "\n",
    "\n",
    "def load_image(image_path: str, output_type: ImageTypeStr = \"pil\") -> ImageType:\n",
    "    image = PIL.Image.open(image_path)\n",
    "    if output_type == \"pil\":\n",
    "        return image\n",
    "    elif output_type == \"np\":\n",
    "        return np.array(image)\n",
    "    elif output_type == \"pt\":\n",
    "        return torch.tensor(np.array(image).transpose(2, 0, 1), dtype=torch.float32)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported output_type: {output_type}\")\n",
    "\n",
    "\n",
    "class CLIPScore:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_or_name_path: str = _DEFAULT_MODEL,\n",
    "        torch_dtype: torch.dtype = _DEFAULT_TORCH_DTYPE,\n",
    "        local_files_only: bool = False,\n",
    "        device: str | torch.device = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.dtype = torch_dtype\n",
    "        self.model = CLIPModel.from_pretrained(model_or_name_path, torch_dtype=torch_dtype, local_files_only=False).to(device)\n",
    "        self.model.eval()\n",
    "        self.processor = CLIPProcessor.from_pretrained(model_or_name_path, local_files_only=False)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_text_features(self, text: str | List[str], *, norm: bool = False) -> torch.Tensor:\n",
    "        if not isinstance(text, list):\n",
    "            text = [text]\n",
    "        inputs = self.processor(text=text, padding=True, return_tensors=\"pt\")\n",
    "        text_features = self.model.get_text_features(\n",
    "            inputs[\"input_ids\"].to(self.device),\n",
    "            inputs[\"attention_mask\"].to(self.device),\n",
    "        )\n",
    "        if norm:\n",
    "            text_features = text_features / text_features.norm(p=2, dim=-1, keepdim=True)\n",
    "        return text_features\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_image_features(self, image: ImageType | List[ImageType], *, norm: bool = False) -> torch.Tensor:\n",
    "        if not isinstance(image, list):\n",
    "            image = [image]\n",
    "        inputs = self.processor(images=image, return_tensors=\"pt\")\n",
    "        image_features = self.model.get_image_features(inputs[\"pixel_values\"].to(self.device, dtype=self.dtype))\n",
    "        if norm:\n",
    "            image_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)\n",
    "        return image_features\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def clipi_score(self, images1: ImageType | List[ImageType], images2: ImageType | List[ImageType]) -> tuple[float, int]:\n",
    "        if not isinstance(images1, list):\n",
    "            images1 = [images1]\n",
    "        if not isinstance(images2, list):\n",
    "            images2 = [images2]\n",
    "        # print(\"image1_num\", len(images1))\n",
    "        assert len(images1) == len(images2) or len(images2) == 1, f\"Number of images1 ({len(images1)}) and images2 ({len(images2)}) should be same.\"\n",
    "        images2_features = self.get_image_features(images2, norm=True)\n",
    "        score = 0\n",
    "        if len(images1) > 1:\n",
    "            for img in images1:\n",
    "                images1_features = self.get_image_features(img, norm=True)\n",
    "                # cosine similarity between feature vectors\n",
    "                score += 100 * (images1_features * images2_features).sum(axis=-1)\n",
    "                # print(score)\n",
    "                # print(\"score:\", score.sum(0))\n",
    "            return score.sum(0).float() / len(images1), len(images1)\n",
    "        else:\n",
    "            images1_features = self.get_image_features(images1, norm=True)\n",
    "            # cosine similarity between feature vectors\n",
    "            score = 100 * (images1_features * images2_features).sum(axis=-1)\n",
    "            return score.sum(0).float(), len(images1)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def clipt_score(self, texts: str | List[str], images: ImageType | List[ImageType]) -> tuple[float, int]:\n",
    "        if not isinstance(texts, list):\n",
    "            texts = [texts]\n",
    "        if not isinstance(images, list):\n",
    "            images = [images]\n",
    "        assert len(texts) == len(images), f\"Number of texts ({len(texts)}) and images ({len(images)}) should be same.\"\n",
    "        texts_features = self.get_text_features(texts, norm=True)\n",
    "        images_features = self.get_image_features(images, norm=True)\n",
    "        # cosine similarity between feature vectors\n",
    "        score = 100 * (texts_features * images_features).sum(axis=-1)\n",
    "        return score.sum(0).float(), len(texts)\n",
    "\n",
    "\n",
    "def single_clipi_score(image1_paths: List[str], image2_paths: List[str], clip_score: CLIPScore) -> float:\n",
    "    #print(\"attention:\",len(image1_paths))\n",
    "    # print(len(image2_paths))\n",
    "    assert len(image1_paths) == len(image2_paths), f\"Number of image1 files {len(image1_paths)} != number of image2 files {len(image2_paths)}.\"\n",
    "\n",
    "    total_score = 0.0\n",
    "    pbar = tqdm(total=len(image1_paths), desc=\"Evaluating CLIP-I Score\")\n",
    "\n",
    "    for image1_path_list, image2_path in zip(image1_paths, image2_paths):\n",
    "        image2 = load_image(image2_path)\n",
    "        image1 = []\n",
    "        for image1_path in image1_path_list:\n",
    "            # print(\"image1_path\", image1_path)\n",
    "            image1.append(load_image(image1_path))\n",
    "\n",
    "        score, _ = clip_score.clipi_score(image1, image2)\n",
    "        # print(score)\n",
    "        total_score += score.item()\n",
    "        pbar.update(1)\n",
    "        # print(\"total_score:\", total_score)\n",
    "\n",
    "    pbar.close()\n",
    "    return total_score / len(image1_paths)\n",
    "\n",
    "\n",
    "def single_clipt_score(texts: List[str], image_paths: List[str], clip_score: CLIPScore) -> float:\n",
    "    assert len(texts) == len(image_paths), f\"Number of texts ({len(texts)}) != number of image files {len(image_paths)}.\"\n",
    "    total_score = 0.0\n",
    "    pbar = tqdm(total=len(texts), desc=\"Evaluating CLIP-T Score\")\n",
    "\n",
    "    for text, image_path in zip(texts, image_paths):\n",
    "        image = load_image(image_path)\n",
    "        score, _ = clip_score.clipt_score(text, image)\n",
    "        total_score += score.item()\n",
    "        pbar.update(1)\n",
    "\n",
    "    pbar.close()\n",
    "    return total_score / len(texts)\n",
    "\n",
    "\n",
    "def clip_eval(mode: Literal[\"clipi\", \"clipt\"], dir1: str, dir2: str):\n",
    "    clip_score = CLIPScore()\n",
    "    if mode == \"clipi\":\n",
    "        image1_paths = glob.glob(os.path.join(dir1, \"*\"))\n",
    "        image2_paths = glob.glob(os.path.join(dir2, \"*\"))\n",
    "        print(f\"CLIP-I Score: {clipi_score(image1_paths, image2_paths, clip_score)}\")\n",
    "    elif mode == \"clipt\":\n",
    "        text_files = glob.glob(os.path.join(dir1, \"*.txt\"))\n",
    "        image_paths = glob.glob(os.path.join(dir2, \"*\"))\n",
    "        texts = []\n",
    "        for text_file in text_files:\n",
    "            with open(text_file, \"r\") as f:\n",
    "                texts.append(f.read().strip())\n",
    "        print(f\"CLIP-T Score: {clipt_score(texts, image_paths, clip_score)}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    result_path = \"/mnt/workspace/ziwei/KBR_results/msdiffusion/level_one\"\n",
    "    json_files = glob.glob(os.path.join(result_path, \"*.json\"))\n",
    "    for json_file in json_files:\n",
    "        with open(json_file, \"r\") as reader:\n",
    "            data_all = json.load(reader)\n",
    "            result_images = []\n",
    "            reference_images = []\n",
    "            texts = []\n",
    "            for data in data_all:\n",
    "                result_images.append(data[\"result_image\"])\n",
    "                reference_images.append(data[\"reference_image\"])\n",
    "                texts.append(data[\"text\"])\n",
    "            # print(reference_images)\n",
    "            # print(result_images)\n",
    "            clipt_score_value = single_clipt_score(texts, result_images, CLIPScore())\n",
    "            clipi_score_value = single_clipi_score(reference_images, result_images, CLIPScore())\n",
    "            print(\"clipt_score:\", clipt_score_value)\n",
    "            # print(\"Stop\")\n",
    "            print(\"clipi_score:\", clipi_score_value)x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating CLIP-T Score: 100%|██████████| 200/200 [00:09<00:00, 21.90it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'read'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/envs/sd3_hzw/lib/python3.10/site-packages/PIL/Image.py:3437\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3436\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3437\u001b[0m     fp\u001b[39m.\u001b[39;49mseek(\u001b[39m0\u001b[39m)\n\u001b[1;32m   3438\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mAttributeError\u001b[39;00m, io\u001b[39m.\u001b[39mUnsupportedOperation):\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'seek'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/mnt/workspace/ziwei/benchmark/KBR-bench/eval/test.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://dsw-gateway.alibaba-inc.com/mnt/workspace/ziwei/benchmark/KBR-bench/eval/test.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=163'>164</a>\u001b[0m     texts\u001b[39m.\u001b[39mappend(data[\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    <a href='vscode-notebook-cell://dsw-gateway.alibaba-inc.com/mnt/workspace/ziwei/benchmark/KBR-bench/eval/test.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=164'>165</a>\u001b[0m clipt_score_value \u001b[39m=\u001b[39m clipt_score(texts, result_images, CLIPScore())\n\u001b[0;32m--> <a href='vscode-notebook-cell://dsw-gateway.alibaba-inc.com/mnt/workspace/ziwei/benchmark/KBR-bench/eval/test.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=165'>166</a>\u001b[0m clipi_score_value \u001b[39m=\u001b[39m clipi_score(reference_images, result_images, CLIPScore())\n\u001b[1;32m    <a href='vscode-notebook-cell://dsw-gateway.alibaba-inc.com/mnt/workspace/ziwei/benchmark/KBR-bench/eval/test.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=166'>167</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mclipt_score:\u001b[39m\u001b[39m\"\u001b[39m, clipt_score_value)\n\u001b[1;32m    <a href='vscode-notebook-cell://dsw-gateway.alibaba-inc.com/mnt/workspace/ziwei/benchmark/KBR-bench/eval/test.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=167'>168</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mclipi_score:\u001b[39m\u001b[39m\"\u001b[39m, clipi_score_value)\n",
      "\u001b[1;32m/mnt/workspace/ziwei/benchmark/KBR-bench/eval/test.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://dsw-gateway.alibaba-inc.com/mnt/workspace/ziwei/benchmark/KBR-bench/eval/test.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=110'>111</a>\u001b[0m pbar \u001b[39m=\u001b[39m tqdm(total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(image1_paths), desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEvaluating CLIP-I Score\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell://dsw-gateway.alibaba-inc.com/mnt/workspace/ziwei/benchmark/KBR-bench/eval/test.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=112'>113</a>\u001b[0m \u001b[39mfor\u001b[39;00m image1_path, image2_path \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(image1_paths, image2_paths):\n\u001b[0;32m--> <a href='vscode-notebook-cell://dsw-gateway.alibaba-inc.com/mnt/workspace/ziwei/benchmark/KBR-bench/eval/test.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=113'>114</a>\u001b[0m     image1 \u001b[39m=\u001b[39m load_image(image1_path)\n\u001b[1;32m    <a href='vscode-notebook-cell://dsw-gateway.alibaba-inc.com/mnt/workspace/ziwei/benchmark/KBR-bench/eval/test.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=114'>115</a>\u001b[0m     image2 \u001b[39m=\u001b[39m load_image(image2_path)\n\u001b[1;32m    <a href='vscode-notebook-cell://dsw-gateway.alibaba-inc.com/mnt/workspace/ziwei/benchmark/KBR-bench/eval/test.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=115'>116</a>\u001b[0m     score, _ \u001b[39m=\u001b[39m clip_score\u001b[39m.\u001b[39mclipi_score(image1, image2)\n",
      "\u001b[1;32m/mnt/workspace/ziwei/benchmark/KBR-bench/eval/test.ipynb Cell 3\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://dsw-gateway.alibaba-inc.com/mnt/workspace/ziwei/benchmark/KBR-bench/eval/test.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_image\u001b[39m(image_path: \u001b[39mstr\u001b[39m, output_type: ImageTypeStr \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpil\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ImageType:\n\u001b[0;32m---> <a href='vscode-notebook-cell://dsw-gateway.alibaba-inc.com/mnt/workspace/ziwei/benchmark/KBR-bench/eval/test.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m     image \u001b[39m=\u001b[39m PIL\u001b[39m.\u001b[39;49mImage\u001b[39m.\u001b[39;49mopen(image_path)\n\u001b[1;32m     <a href='vscode-notebook-cell://dsw-gateway.alibaba-inc.com/mnt/workspace/ziwei/benchmark/KBR-bench/eval/test.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m     \u001b[39mif\u001b[39;00m output_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpil\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell://dsw-gateway.alibaba-inc.com/mnt/workspace/ziwei/benchmark/KBR-bench/eval/test.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m image\n",
      "File \u001b[0;32m/opt/conda/envs/sd3_hzw/lib/python3.10/site-packages/PIL/Image.py:3439\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3437\u001b[0m     fp\u001b[39m.\u001b[39mseek(\u001b[39m0\u001b[39m)\n\u001b[1;32m   3438\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mAttributeError\u001b[39;00m, io\u001b[39m.\u001b[39mUnsupportedOperation):\n\u001b[0;32m-> 3439\u001b[0m     fp \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mBytesIO(fp\u001b[39m.\u001b[39;49mread())\n\u001b[1;32m   3440\u001b[0m     exclusive_fp \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   3442\u001b[0m prefix \u001b[39m=\u001b[39m fp\u001b[39m.\u001b[39mread(\u001b[39m16\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'read'"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "from typing import Literal, List\n",
    "import fire\n",
    "import megfile\n",
    "import torch\n",
    "import PIL.Image\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "import glob\n",
    "from typing import Literal, TypeAlias\n",
    "import json\n",
    "\n",
    "# 定义常量\n",
    "_DEFAULT_MODEL = \"/mnt/workspace/ziwei/checkpoints/clip-vit-base-patch32\"\n",
    "_DEFAULT_TORCH_DTYPE: torch.dtype = torch.float32\n",
    "IMAGE_EXT_LOWER = [\"png\", \"jpeg\", \"jpg\"]\n",
    "IMAGE_EXT = IMAGE_EXT_LOWER + [_ext.upper() for _ext in IMAGE_EXT_LOWER]\n",
    "\n",
    "\"\"\"\n",
    "- pil: `PIL.Image.Image`, size (w, h), seamless conversion between `uint8`\n",
    "- np: `np.ndarray`, shape (h, w, c), default `np.uint8`\n",
    "- pt: `torch.Tensor`, shape (c, h, w), default `torch.uint8`\n",
    "\"\"\"\n",
    "ImageType: TypeAlias = PIL.Image.Image | np.ndarray | torch.Tensor\n",
    "ImageTypeStr: TypeAlias = Literal[\"pil\", \"np\", \"pt\"]\n",
    "ImageFormat: TypeAlias = Literal[\"JPEG\", \"PNG\"]\n",
    "DataFormat: TypeAlias = Literal[\"255\", \"01\", \"11\"]\n",
    "\n",
    "\n",
    "def load_image(image_path: str, output_type: ImageTypeStr = \"pil\") -> ImageType:\n",
    "    image = PIL.Image.open(image_path)\n",
    "    if output_type == \"pil\":\n",
    "        return image\n",
    "    elif output_type == \"np\":\n",
    "        return np.array(image)\n",
    "    elif output_type == \"pt\":\n",
    "        return torch.tensor(np.array(image).transpose(2, 0, 1), dtype=torch.float32)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported output_type: {output_type}\")\n",
    "\n",
    "\n",
    "class CLIPScore:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_or_name_path: str = _DEFAULT_MODEL,\n",
    "        torch_dtype: torch.dtype = _DEFAULT_TORCH_DTYPE,\n",
    "        local_files_only: bool = False,\n",
    "        device: str | torch.device = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.dtype = torch_dtype\n",
    "        self.model = CLIPModel.from_pretrained(model_or_name_path, torch_dtype=torch_dtype, local_files_only=False).to(device)\n",
    "        self.model.eval()\n",
    "        self.processor = CLIPProcessor.from_pretrained(model_or_name_path, local_files_only=False)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_text_features(self, text: str | List[str], *, norm: bool = False) -> torch.Tensor:\n",
    "        if not isinstance(text, list):\n",
    "            text = [text]\n",
    "        inputs = self.processor(text=text, padding=True, return_tensors=\"pt\")\n",
    "        text_features = self.model.get_text_features(\n",
    "            inputs[\"input_ids\"].to(self.device),\n",
    "            inputs[\"attention_mask\"].to(self.device),\n",
    "        )\n",
    "        if norm:\n",
    "            text_features = text_features / text_features.norm(p=2, dim=-1, keepdim=True)\n",
    "        return text_features\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_image_features(self, image: ImageType | List[ImageType], *, norm: bool = False) -> torch.Tensor:\n",
    "        if not isinstance(image, list):\n",
    "            image = [image]\n",
    "        inputs = self.processor(images=image, return_tensors=\"pt\")\n",
    "        image_features = self.model.get_image_features(inputs[\"pixel_values\"].to(self.device, dtype=self.dtype))\n",
    "        if norm:\n",
    "            image_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)\n",
    "        return image_features\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def clipi_score(self, images1: ImageType | List[ImageType], images2: ImageType | List[ImageType]) -> tuple[float, int]:\n",
    "        if not isinstance(images1, list):\n",
    "            images1 = [images1]\n",
    "        if not isinstance(images2, list):\n",
    "            images2 = [images2]\n",
    "        assert len(images1) == len(images2), f\"Number of images1 ({len(images1)}) and images2 ({len(images2)}) should be same.\"\n",
    "        images1_features = self.get_image_features(images1, norm=True)\n",
    "        images2_features = self.get_image_features(images2, norm=True)\n",
    "        # cosine similarity between feature vectors\n",
    "        score = 100 * (images1_features * images2_features).sum(axis=-1)\n",
    "        return score.sum(0).float(), len(images1)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def clipt_score(self, texts: str | List[str], images: ImageType | List[ImageType]) -> tuple[float, int]:\n",
    "        if not isinstance(texts, list):\n",
    "            texts = [texts]\n",
    "        if not isinstance(images, list):\n",
    "            images = [images]\n",
    "        assert len(texts) == len(images), f\"Number of texts ({len(texts)}) and images ({len(images)}) should be same.\"\n",
    "        texts_features = self.get_text_features(texts, norm=True)\n",
    "        images_features = self.get_image_features(images, norm=True)\n",
    "        # cosine similarity between feature vectors\n",
    "        score = 100 * (texts_features * images_features).sum(axis=-1)\n",
    "        return score.sum(0).float(), len(texts)\n",
    "\n",
    "def clipi_score(image1_paths: List[str], image2_paths: List[str], clip_score: CLIPScore) -> float:\n",
    "    assert len(image1_paths) == len(image2_paths), f\"Number of image1 files {len(image1_paths)} != number of image2 files {len(image2_paths)}.\"\n",
    "    total_score = 0.0\n",
    "    pbar = tqdm(total=len(image1_paths), desc=\"Evaluating CLIP-I Score\")\n",
    "\n",
    "    for image1_path, image2_path in zip(image1_paths, image2_paths):\n",
    "        image1 = load_image(image1_path)\n",
    "        image2 = load_image(image2_path)\n",
    "        score, _ = clip_score.clipi_score(image1, image2)\n",
    "        total_score += score.item()\n",
    "        pbar.update(1)\n",
    "\n",
    "    pbar.close()\n",
    "    return total_score / len(image1_paths)\n",
    "\n",
    "def clipt_score(texts: List[str], image_paths: List[str], clip_score: CLIPScore) -> float:\n",
    "    assert len(texts) == len(image_paths), f\"Number of texts ({len(texts)}) != number of image files {len(image_paths)}.\"\n",
    "    total_score = 0.0\n",
    "    pbar = tqdm(total=len(texts), desc=\"Evaluating CLIP-T Score\")\n",
    "\n",
    "    for text, image_path in zip(texts, image_paths):\n",
    "        image = load_image(image_path)\n",
    "        score, _ = clip_score.clipt_score(text, image)\n",
    "        total_score += score.item()\n",
    "        pbar.update(1)\n",
    "\n",
    "    pbar.close()\n",
    "    return total_score / len(texts)\n",
    "\n",
    "def clip_eval(mode: Literal[\"clipi\", \"clipt\"], dir1: str, dir2: str):\n",
    "    clip_score = CLIPScore()\n",
    "    if mode == \"clipi\":\n",
    "        image1_paths = glob.glob(os.path.join(dir1, '*'))\n",
    "        image2_paths = glob.glob(os.path.join(dir2, '*'))\n",
    "        print(f\"CLIP-I Score: {clipi_score(image1_paths, image2_paths, clip_score)}\")\n",
    "    elif mode == \"clipt\":\n",
    "        text_files = glob.glob(os.path.join(dir1, '*.txt'))\n",
    "        image_paths = glob.glob(os.path.join(dir2, '*'))\n",
    "        texts = []\n",
    "        for text_file in text_files:\n",
    "            with open(text_file, 'r') as f:\n",
    "                texts.append(f.read().strip())\n",
    "        print(f\"CLIP-T Score: {clipt_score(texts, image_paths, clip_score)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    result_path = \"/mnt/workspace/ziwei/SSR_ENcoder/SSR_Encoder/results/KBR_bench/level_one/\"\n",
    "    json_files = glob.glob(os.path.join(result_path, \"*.json\"))\n",
    "    for json_file in json_files:\n",
    "        with open(json_file, \"r\") as reader:\n",
    "            data_all = json.load(reader)\n",
    "            result_images = []\n",
    "            reference_images = []\n",
    "            texts = []\n",
    "            for data in data_all:\n",
    "                result_images.append(data[\"result_image\"])\n",
    "                reference_images.append(data[\"reference_image\"])\n",
    "                texts.append(data[\"text\"])\n",
    "            clipt_score_value = clipt_score(texts, result_images, CLIPScore())\n",
    "            clipi_score_value = clipi_score(reference_images, result_images, CLIPScore())\n",
    "            print(\"clipt_score:\", clipt_score_value)\n",
    "            print(\"clipi_score:\", clipi_score_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple\n",
      "Collecting megfile\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/86/09/2c831aea77c8a37c565813087b841d1f63aa776d09e84c27be6654234843/megfile-3.1.0-py3-none-any.whl (135 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m232.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting boto3 (from megfile)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/cd/65/7cf1fd8f8073b1884fe7e559acb3fdd9957ba0cecd46e8bd4b2844b0a48a/boto3-1.34.143-py3-none-any.whl (139 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m232.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting botocore>=1.13.0 (from megfile)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/cf/15/359c52942418b5b9c094909174b9e97dd0364d1fc49fa588cab9361ae0fe/botocore-1.34.143-py3-none-any.whl (12.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m239.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /opt/conda/envs/sd3_hzw/lib/python3.10/site-packages (from megfile) (2.32.3)\n",
      "Collecting paramiko (from megfile)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/ad/50/8792484502c8141c20c996b802fefa8435a9c018a2bb440a06b172782118/paramiko-3.4.0-py3-none-any.whl (225 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.9/225.9 kB\u001b[0m \u001b[31m244.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/envs/sd3_hzw/lib/python3.10/site-packages (from megfile) (4.66.4)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/envs/sd3_hzw/lib/python3.10/site-packages (from megfile) (6.0.1)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from botocore>=1.13.0->megfile)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/31/b4/b9b800c45527aadd64d5b442f9b932b00648617eb5d63d2c7a6587b7cafc/jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/envs/sd3_hzw/lib/python3.10/site-packages (from botocore>=1.13.0->megfile) (2.9.0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /opt/conda/envs/sd3_hzw/lib/python3.10/site-packages (from botocore>=1.13.0->megfile) (2.2.2)\n",
      "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3->megfile)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/3c/4a/b221409913760d26cf4498b7b1741d510c82d3ad38381984a3ddc135ec66/s3transfer-0.10.2-py3-none-any.whl (82 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.7/82.7 kB\u001b[0m \u001b[31m238.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting bcrypt>=3.2 (from paramiko->megfile)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/97/00/21e34b365b895e6faf9cc5d4e7b97dd419e08f8a7df119792ec206b4a3fa/bcrypt-4.1.3-cp39-abi3-manylinux_2_28_x86_64.whl (283 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.7/283.7 kB\u001b[0m \u001b[31m235.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting cryptography>=3.3 (from paramiko->megfile)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/fa/e2/b7e6e8c261536c489d9cf908769880d94bd5d9a187e166b0dc838d2e6a56/cryptography-42.0.8-cp39-abi3-manylinux_2_28_x86_64.whl (3.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m234.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pynacl>=1.5 (from paramiko->megfile)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/ee/87/f1bb6a595f14a327e8285b9eb54d41fef76c585a0edef0a45f6fc95de125/PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (856 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m856.7/856.7 kB\u001b[0m \u001b[31m232.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/sd3_hzw/lib/python3.10/site-packages (from requests->megfile) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/sd3_hzw/lib/python3.10/site-packages (from requests->megfile) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/sd3_hzw/lib/python3.10/site-packages (from requests->megfile) (2024.7.4)\n",
      "Collecting cffi>=1.12 (from cryptography>=3.3->paramiko->megfile)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/c9/7c/43d81bdd5a915923c3bad5bb4bff401ea00ccc8e28433fb6083d2e3bf58e/cffi-1.16.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (443 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.9/443.9 kB\u001b[0m \u001b[31m231.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /opt/conda/envs/sd3_hzw/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore>=1.13.0->megfile) (1.16.0)\n",
      "Collecting pycparser (from cffi>=1.12->cryptography>=3.3->paramiko->megfile)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/13/a3/a812df4e2dd5696d1f351d58b8fe16a405b234ad2886a0dab9183fb78109/pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.6/117.6 kB\u001b[0m \u001b[31m231.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pycparser, jmespath, bcrypt, cffi, botocore, s3transfer, pynacl, cryptography, paramiko, boto3, megfile\n",
      "Successfully installed bcrypt-4.1.3 boto3-1.34.143 botocore-1.34.143 cffi-1.16.0 cryptography-42.0.8 jmespath-1.0.1 megfile-3.1.0 paramiko-3.4.0 pycparser-2.22 pynacl-1.5.0 s3transfer-0.10.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install megfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a city park on a morning , a dog is playfully nudging a topper hat near the Cloud Gate .\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "\n",
    "doc=nlp(\"In a city park on a cloudy morning, a white wheaten terrier dog is playfully nudging a white topper hat near the Cloud Gate.\")\n",
    "prompt_without_adj=' '.join([token.text for token in doc if token.pos_ != 'ADJ']) #remove adj\n",
    "print(prompt_without_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple\n",
      "Processing ./en_core_web_sm-2.3.0.tar.gz\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting spacy<2.4.0,>=2.3.0 (from en_core_web_sm==2.3.0)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/c8/7c/bb5b8683efd1d36a1d415e426aa95030a7c6f1b2aab492f8403b85560ec4/spacy-2.3.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/4.9 MB\u001b[0m \u001b[31m222.7 kB/s\u001b[0m eta \u001b[36m0:00:12\u001b[0m^C\n",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/4.9 MB\u001b[0m \u001b[31m222.7 kB/s\u001b[0m eta \u001b[36m0:00:12\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sd3_hzw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
